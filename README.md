# RL Benchmarks: A Historic Recollection of Reinforcement Learning Benchmarks


Adams, S., Arel, I., Bach, J., Coop, R., Furlan, R., Goertzel, B., Hall, J. S., Samsonovich, A., Scheutz, M., Schlesinger, M., Shapiro, S. C., & Sowa, J. (2012). Mapping the landscape of human-level artificial general intelligence. AI Magazine, 33(1), 25–42. 
[[paper]](https://doi.org/10.1609/aimag.v33i1.2322)

Agarwal, R., Schuurmans, D., & Norouzi, M. (2019). An Optimistic Perspective on Offline Reinforcement Learning. In arXiv [cs.LG]. arXiv. [[paper]](http://arxiv.org/abs/1907.04543)

Ahn, M., Zhu, H., Hartikainen, K., Ponte, H., Gupta, A., Levine, S., & Kumar, V. (2019). ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots. In arXiv [cs.RO]. arXiv. [[paper]](http://arxiv.org/abs/1909.11639)

Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton, A., Gaffney, S., King, H., Hassabis, D., … Petersen, S. (2016). DeepMind Lab. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1612.03801

Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2012). The Arcade Learning Environment: An Evaluation Platform for General Agents. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1207.4708

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). OpenAI Gym. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1606.01540

Chollet, F. (2019). On the Measure of Intelligence. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1911.01547

Cobbe, K., Hesse, C., Hilton, J., & Schulman, J. (2019). Leveraging Procedural Generation to Benchmark Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1912.01588

Cobbe, K., Klimov, O., Hesse, C., Kim, T., & Schulman, J. (2018). Quantifying Generalization in Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1812.02341

Coleman, O. J., Blair, A. D., & Clune, J. (2014). Automated generation of environments to test the general learning capabilities of AI agents. Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, 161–168. https://doi.org/10.1145/2576768.2598257

Collins, J., McVicar, J., Wedlock, D., Brown, R., Howard, D., & Leitner, J. (2019). Benchmarking Simulated Robotic Manipulation through a Real World Dataset. In arXiv [cs.RO]. arXiv. http://arxiv.org/abs/1911.01557

Côté, M.-A., Kádár, Á., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., Tay, W., & Trischler, A. (2019). TextWorld: A Learning Environment for Text-Based Games. Computer Games, 41–75. https://doi.org/10.1007/978-3-030-24337-1_3

Crosby, M., Beyret, B., Shanahan, M., Hernández-Orallo, J., Cheke, L., & Halina, M. (2020). The Animal-AI Testbed and Competition. In H. J. Escalante & R. Hadsell (Eds.), Proceedings of the NeurIPS 2019 Competition and Demonstration Track (Vol. 123, pp. 164–176). PMLR. https://proceedings.mlr.press/v123/crosby20a.html

Daniel Freeman, C., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., & Bachem, O. (2021). Brax -- A Differentiable Physics Engine for Large Scale Rigid Body Simulation. In arXiv [cs.RO]. arXiv. http://arxiv.org/abs/2106.13281

Duan, Y., Chen, X., Houthooft, R., Schulman, J., & Abbeel, P. (2016). Benchmarking Deep Reinforcement Learning for Continuous Control. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1604.06778

Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru, C., Gowal, S., & Hester, T. (2020). An empirical investigation of the challenges of real-world reinforcement learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2003.11881

Fan, L., & Zhu, Y. (2018). SURREAL: Open-source reinforcement learning framework and robot manipulation benchmark. https://surreal.stanford.edu/img/surreal-corl2018.pdf

Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Badia, A. P., Buttimore, G., Deck, C., Leibo, J. Z., & Blundell, C. (2019). Generalization of Reinforcement Learners with Working and Episodic Memory. Advances in Neural Information Processing Systems, 12448–12457.

Fu, J., Kumar, A., Nachum, O., Tucker, G., & Levine, S. (2020). D4RL: Datasets for Deep Data-Driven Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2004.07219

Fujimoto, S., Conti, E., Ghavamzadeh, M., & Pineau, J. (2019). Benchmarking Batch Deep Reinforcement Learning Algorithms. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1910.01708

Gulcehre, C., Wang, Z., Novikov, A., Paine, T. L., Colmenarejo, S. G., Zolna, K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., Dulac-Arnold, G., Li, J., Norouzi, M., Hoffman, M., Nachum, O., Tucker, G., Heess, N., & de Freitas, N. (2020). RL Unplugged: A suite of benchmarks for offline reinforcement learning. In arXiv [cs.LG]. arXiv. https://proceedings.neurips.cc/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf

Guss, W. H., Codel, C., Hofmann, K., Houghton, B., Kuno, N., Milani, S., Mohanty, S., Liebana, D. P., Salakhutdinov, R., Topin, N., Veloso, M., & Wang, P. (2019). The MineRL 2019 Competition on Sample Efficient Reinforcement Learning using Human Priors. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1904.10079

Guss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., & Salakhutdinov, R. (2019). MineRL: A Large-Scale Dataset of Minecraft Demonstrations. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1907.13440

Hernandez-Orallo, J. (2017). The measure of all minds: Evaluating natural and artificial intelligence. Cambridge University Press. https://doi.org/10.1017/9781316594179

James, S., Ma, Z., Arrojo, D. R., & Davison, A. J. (2019). RLBench: The Robot Learning Benchmark & Learning Environment. In arXiv [cs.RO]. arXiv. http://arxiv.org/abs/1909.12271

Johnson, M., Hofmann, K., Hutton, T., & Bignell, D. (2016). The Malmo platform for artificial intelligence experimentation. Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, 4246–4247. https://dl.acm.org/doi/10.5555/3061053.3061259

Juliani, A., Berges, V.-P., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., & Lange, D. (2018). Unity: A General Platform for Intelligent Agents. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1809.02627

Juliani, A., Khalifa, A., Berges, V.-P., Harper, J., Teng, E., Henry, H., Crespi, A., Togelius, J., & Lange, D. (2019). Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1902.01378

Kannan, H., Hafner, D., Finn, C., & Erhan, D. (2021). RoboDesk environment v0. https://github.com/google-research/robodesk

Kempka, M., Wydmuch, M., Runc, G., Toczek, J., & Jaśkowski, W. (2016). ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1605.02097

Kurach, K., Raichuk, A., Stańczyk, P., Zając, M., Bachem, O., Espeholt, L., Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., & Gelly, S. (2019). Google Research Football: A Novel Reinforcement Learning Environment. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1907.11180

Küttler, H., Nardelli, N., Miller, A. H., Raileanu, R., Selvatici, M., Grefenstette, E., & Rocktäschel, T. (2020). The NetHack Learning Environment. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2006.13760

Laird, J. E., & Wray, R. E., III. (2010). Cognitive Architecture Requirements for Achieving AGI. Proceedings of the 3d Conference on Artificial General Intelligence (AGI-10). 3d Conference on Artificial General Intelligence (AGI-10), Lugano, Switzerland. https://doi.org/10.2991/agi.2010.2

Le Paine, T., Gulcehre, C., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., Tanburn, R., Kapturowski, S., Rabinowitz, N., Williams, D., Barth-Maron, G., Wang, Z., de Freitas, N., & Worlds Team. (2019). Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1909.01387

Lee, Y., Hu, E. S., Yang, Z., Yin, A., & Lim, J. J. (2019). IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks. In arXiv [cs.RO]. arXiv. http://arxiv.org/abs/1911.07246

Leibo, J. Z., de Masson d’Autume, C., Zoran, D., Amos, D., Beattie, C., Anderson, K., Castañeda, A. G., Sanchez, M., Green, S., Gruslys, A., Legg, S., Hassabis, D., & Botvinick, M. M. (2018). Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1801.08116

Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the arcade learning environment: evaluation protocols and open problems for general agents. The Journal of Artificial Intelligence Research, 61(1), 523–562. https://dl.acm.org/doi/abs/10.5555/3241691.3241702

Mbuwir, B. V., Manna, C., Spiessens, F., & Deconinck, G. (2020). Benchmarking reinforcement learning algorithms for demand response applications. 2020 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe), 289–293. https://doi.org/10.1109/ISGT-Europe47291.2020.9248800

Nichol, A., Pfau, V., Hesse, C., Klimov, O., & Schulman, J. (2018). Gotta Learn Fast: A New Benchmark for Generalization in RL. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1804.03720

Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore, T., Szepesvari, C., Singh, S., Van Roy, B., Sutton, R., Silver, D., & Van Hasselt, H. (2019). Behaviour Suite for Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1908.03568

Perez-Liebana, D., Samothrakis, S., Togelius, J., Schaul, T., Lucas, S. M., Couëtoux, A., Lee, J., Lim, C.-U., & Thompson, T. (2016). The 2014 General Video Game Playing Competition. IEEE Transactions on Computational Intelligence in AI and Games, 8(3), 229–243. https://doi.org/10.1109/TCIAIG.2015.2402393

Platanios, E. A., Saparov, A., & Mitchell, T. (2020). Jelly Bean World: A Testbed for Never-Ending Learning. https://www.semanticscholar.org/paper/665fbb2645d1213e7eb95d870acd2ed75c74d1a5

Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., & Levine, S. (2017). Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1709.10087

Ray, A., Achiam, J., & Amodei, D. (2019). Benchmarking safe exploration in deep reinforcement learning. https://cdn.openai.com/safexp-short.pdf

Riedmiller, M., Blum, M., Lampe, T., Hafner, R., Lange, S., & Timmer, S. (2013). CLSquare: Closed Loop Simulation System. https://ml.informatik.uni-freiburg.de/former/research/clsquare.html

robogym: Robotics Gym Environments. (n.d.). Github. Retrieved September 1, 2021, from https://github.com/openai/robogym (Original work published 2020)

Samvelyan, M., Kirk, R., Kurin, V., Parker-Holder, J., Jiang, M., Hambro, E., Petroni, F., Kuttler, H., Grefenstette, E., & Rocktäschel, T. (2021). MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research. https://openreview.net/pdf?id=skFwlyefkWJ

Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., & Batra, D. (2019). Habitat: A Platform for Embodied AI Research. In arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1904.01201

Schaul, T. (2013, August). A video game description language for model-based or interactive learning. 2013 IEEE Conference on Computational Inteligence in Games (CIG). 2013 IEEE Conference on Computational Intelligence and Games (CIG), Niagara Falls, ON, Canada. https://doi.org/10.1109/cig.2013.6633610

Schaul, T., Togelius, J., & Schmidhuber, J. (2011). Measuring Intelligence through Games. In arXiv [cs.AI]. arXiv. http://arxiv.org/abs/1109.1314

Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam, M., Chaplot, D., Maksymets, O., Gokaslan, A., Vondrus, V., Dharur, S., Meier, F., Galuba, W., Chang, A., Kira, Z., Koltun, V., Malik, J., … Batra, D. (2021). Habitat 2.0: Training Home Assistants to Rearrange their Habitat. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2106.14405

Tanner, B., & White, A. (2009). RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments. Journal of Machine Learning Research: JMLR, 10(74), 2133–2136. http://jmlr.org/papers/v10/tanner09a.html

Tassa, Y., Tunyasuvunakool, S., Muldal, A., Doron, Y., Trochim, P., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., & Heess, N. (2020). dm_control: Software and Tasks for Continuous Control. In arXiv [cs.RO]. arXiv. http://arxiv.org/abs/2006.12983

Wang, J. X., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck, C., Choy, P., Cassin, M., Reynolds, M., Song, F., Buttimore, G., Reichert, D. P., Rabinowitz, N., Matthey, L., Hassabis, D., Lerchner, A., & Botvinick, M. (2021). Alchemy: A structured task distribution for meta-reinforcement learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.02926

Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., & Ba, J. (2019). Benchmarking Model-Based Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1907.02057

Whiteson, S., Tanner, B., Taylor, M. E., & Stone, P. (2011, April). Protecting against evaluation overfitting in empirical reinforcement learning. 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL). 2011 Ieee Symposium On Adaptive Dynamic Programming And Reinforcement Learning, Paris, France. https://doi.org/10.1109/adprl.2011.5967363

Whiteson, S., Tanner, B., & White, A. (2010). The reinforcement learning competitions. AI Magazine. http://citeseerx.ist.psu.edu/viewdoc/citations;jsessionid=B401441AFD41A5C593498EFD3ACB31EF?doi=10.1.1.634.5825

Yu, T., Quillen, D., He, Z., Julian, R., Narayan, A., Shively, H., Bellathur, A., Hausman, K., Finn, C., & Levine, S. (2019). Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1910.10897

Zhang, A., Wu, Y., & Pineau, J. (2018). Natural Environment Benchmarks for Reinforcement Learning. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1811.06032
